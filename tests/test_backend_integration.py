# tests/test_backend_integration.py
from config import get_config
from core.pipeline import CityAssistantPipeline
import asyncio
import pytest
import sys
from pathlib import Path

# –î–æ–±–∞–≤–ª—è–µ–º –∫–æ—Ä–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞ –≤ PYTHONPATH
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))


class TestBackendIntegration:
    """üß™ –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï –ò–ù–¢–ï–ì–†–ê–¶–ò–ò BACKEND-–ö–û–ú–ü–û–ù–ï–ù–¢–û–í"""

    @pytest.fixture
    async def pipeline(self):
        """–°–æ–∑–¥–∞–Ω–∏–µ —Ç–µ—Å—Ç–æ–≤–æ–≥–æ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
        config = get_config('testing')
        pipeline = CityAssistantPipeline(config)
        await pipeline.initialize()
        yield pipeline
        # Cleanup –º–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ø—Ä–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏

    @pytest.mark.asyncio
    async def test_pipeline_initialization(self, pipeline):
        """–¢–µ—Å—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
        assert pipeline._initialized == True
        assert 'toxicity_filter' in pipeline.components
        assert 'dialog_manager' in pipeline.components
        assert 'vector_store' in pipeline.components
        assert 'llm_client' in pipeline.components

    @pytest.mark.asyncio
    async def test_user_query_processing(self, pipeline):
        """–¢–µ—Å—Ç –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
        result = await pipeline.process_user_query(
            user_id="test_user_1",
            query="–ö–∞–∫ –ø–æ–ª—É—á–∏—Ç—å –ø–∞—Å–ø–æ—Ä—Ç?"
        )

        assert result.status.value == "success"
        assert len(result.response) > 0
        assert result.processing_time > 0
        assert "–ø–∞—Å–ø–æ—Ä—Ç" in result.response.lower()

    @pytest.mark.asyncio
    async def test_dialog_context_persistence(self, pipeline):
        """–¢–µ—Å—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –¥–∏–∞–ª–æ–≥–∞"""
        user_id = "test_user_2"

        # –ü–µ—Ä–≤—ã–π –∑–∞–ø—Ä–æ—Å
        result1 = await pipeline.process_user_query(user_id, "–ü—Ä–∏–≤–µ—Ç")
        dialog1 = await pipeline.components['dialog_manager'].get_dialog_context(user_id)

        # –í—Ç–æ—Ä–æ–π –∑–∞–ø—Ä–æ—Å
        result2 = await pipeline.process_user_query(user_id, "–ö–∞–∫ –¥–µ–ª–∞?")
        dialog2 = await pipeline.components['dialog_manager'].get_dialog_context(user_id)

        assert len(dialog2.messages) > len(dialog1.messages)
        assert dialog2.total_tokens > dialog1.total_tokens

    @pytest.mark.asyncio
    async def test_performance_benchmark(self, pipeline):
        """–¢–µ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞"""
        import time

        start_time = time.time()
        queries = [
            "–ö–∞–∫ –ø–æ–ª—É—á–∏—Ç—å –ø–∞—Å–ø–æ—Ä—Ç?",
            "–ö–∞–∫–∏–µ –¥–æ–∫—É–º–µ–Ω—Ç—ã –¥–ª—è —Å—É–±—Å–∏–¥–∏–∏?",
            "–ö–∞–∫ –∑–∞–ø–∏—Å–∞—Ç—å—Å—è –∫ –≤—Ä–∞—á—É?",
            "–ì–¥–µ –Ω–∞–π—Ç–∏ –ú–§–¶?"
        ]

        results = []
        for query in queries:
            result = await pipeline.process_user_query(f"perf_user_{queries.index(query)}", query)
            results.append(result)

        total_time = time.time() - start_time
        avg_time = total_time / len(queries)

        print(f"‚è±Ô∏è  –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ–±—Ä–∞–±–æ—Ç–∫–∏: {avg_time:.2f} —Å–µ–∫—É–Ω–¥")

        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –º–µ–Ω—å—à–µ 5 —Å–µ–∫—É–Ω–¥ (–¥–ª—è mock-–æ–≤)
        assert avg_time < 5.0

        # –í—Å–µ –∑–∞–ø—Ä–æ—Å—ã –¥–æ–ª–∂–Ω—ã –±—ã—Ç—å —É—Å–ø–µ—à–Ω—ã–º–∏
        assert all(r.status.value == "success" for r in results)


# –ó–∞–ø—É—Å–∫ —Ç–µ—Å—Ç–æ–≤ –∏–∑ –∫–æ–º–∞–Ω–¥–Ω–æ–π —Å—Ç—Ä–æ–∫–∏
if __name__ == "__main__":
    pytest.main([__file__, "-v"])
